{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbeed8e7",
   "metadata": {},
   "source": [
    "# 🤗 Hugging Face Datasets 완전정복\n",
    "\n",
    "## 목차\n",
    "1. [Datasets 라이브러리 소개](#1-datasets-라이브러리-소개)\n",
    "2. [허브에서 데이터셋 로드하기](#2-허브에서-데이터셋-로드하기)\n",
    "3. [데이터셋 생성하기](#3-데이터셋-생성하기)\n",
    "4. [데이터셋 활용하기](#4-데이터셋-활용하기)\n",
    "5. [데이터 전처리](#5-데이터-전처리)\n",
    "6. [데이터셋 공유하기](#6-데이터셋-공유하기)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Datasets 라이브러리 소개\n",
    "\n",
    "🤗 Datasets는 Hugging Face에서 제공하는 데이터셋 처리 라이브러리입니다.\n",
    "\n",
    "### 주요 특징\n",
    "- **메모리 효율성**: Apache Arrow 기반으로 대용량 데이터도 효율적으로 처리\n",
    "- **빠른 처리 속도**: 최적화된 데이터 로딩 및 처리\n",
    "- **다양한 형식 지원**: 텍스트, 이미지, 오디오, 비디오 등\n",
    "- **스트리밍 지원**: 큰 데이터셋을 메모리에 모두 로드하지 않고 처리 가능\n",
    "- **Hub 연동**: Hugging Face Hub의 수천 개 데이터셋에 쉽게 접근\n",
    "\n",
    "### 설치 방법\n",
    "```bash\n",
    "pip install datasets\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 및 import\n",
    "%pip install datasets transformers torch\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names, get_dataset_config_names\n",
    "from datasets import Dataset, DatasetDict, IterableDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Datasets 버전: {datasets.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596ff89",
   "metadata": {},
   "source": [
    "## 2. 허브에서 데이터셋 로드하기\n",
    "\n",
    "Hugging Face Hub에는 NLP, Computer Vision, Audio 등 다양한 분야의 데이터셋이 수천 개 제공됩니다.\n",
    "\n",
    "### 2.1 데이터셋 정보 확인하기\n",
    "\n",
    "데이터셋을 다운로드하기 전에 `load_dataset_builder()`를 사용해 정보를 미리 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1576e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정보 확인하기\n",
    "ds_builder = load_dataset_builder(\"imdb\")\n",
    "\n",
    "print(\"=== 데이터셋 설명 ===\")\n",
    "print(ds_builder.info.description[:300] + \"...\")\n",
    "print()\n",
    "\n",
    "print(\"=== 데이터셋 특성(Features) ===\")\n",
    "print(ds_builder.info.features)\n",
    "print()\n",
    "\n",
    "print(\"=== 데이터셋 크기 ===\")\n",
    "print(f\"총 크기: {ds_builder.info.dataset_size}\")\n",
    "print(f\"다운로드 크기: {ds_builder.info.download_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e443907",
   "metadata": {},
   "source": [
    "### 2.2 스플릿(Split) 확인하기\n",
    "\n",
    "대부분의 데이터셋은 train, validation, test로 나뉘어 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋의 스플릿 확인\n",
    "splits = get_dataset_split_names(\"imdb\")\n",
    "print(\"사용 가능한 스플릿:\", splits)\n",
    "\n",
    "# 특정 스플릿만 로드\n",
    "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "print(f\"\\n훈련 데이터셋: {train_dataset}\")\n",
    "print(f\"데이터 개수: {len(train_dataset)}\")\n",
    "\n",
    "# 첫 번째 샘플 확인\n",
    "print(\"\\n=== 첫 번째 샘플 ===\")\n",
    "print(f\"텍스트: {train_dataset[0]['text'][:200]}...\")\n",
    "print(f\"라벨: {train_dataset[0]['label']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋 로드 (모든 스플릿)\n",
    "full_dataset = load_dataset(\"imdb\")\n",
    "print(\"전체 데이터셋:\")\n",
    "print(full_dataset)\n",
    "print()\n",
    "\n",
    "# 각 스플릿별 데이터 개수 확인\n",
    "for split_name, split_data in full_dataset.items():\n",
    "    print(f\"{split_name}: {len(split_data):,}개\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec659a0c",
   "metadata": {},
   "source": [
    "### 2.3 구성(Configuration) 확인하기\n",
    "\n",
    "일부 데이터셋은 여러 하위 데이터셋(구성)을 포함합니다. 예: 다국어 데이터셋\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819544bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다국어 데이터셋 예제 - PolyAI/minds14\n",
    "try:\n",
    "    configs = get_dataset_config_names(\"PolyAI/minds14\")\n",
    "    print(\"PolyAI/minds14 데이터셋의 구성들:\")\n",
    "    print(configs[:10])  # 처음 10개만 출력\n",
    "    \n",
    "    # 특정 언어 구성 로드\n",
    "    minds_en = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train[:100]\")  # 처음 100개만\n",
    "    print(f\"\\n영어(en-US) 구성: {minds_en}\")\n",
    "    print(f\"첫 번째 샘플: {minds_en[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"에러 발생: {e}\")\n",
    "    print(\"다른 예제로 진행합니다...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c825ed",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 생성하기\n",
    "\n",
    "자신만의 데이터로 데이터셋을 생성하는 방법을 알아봅시다.\n",
    "\n",
    "### 3.1 파일 기반 빌더\n",
    "\n",
    "CSV, JSON, Parquet, TXT 등 다양한 파일 형식을 지원합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 CSV 데이터 생성\n",
    "import csv\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 임시 CSV 파일 생성\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "csv_file = os.path.join(temp_dir, \"sample_data.csv\")\n",
    "\n",
    "# 샘플 데이터 작성\n",
    "sample_data = [\n",
    "    [\"text\", \"label\"],\n",
    "    [\"이 영화는 정말 재미있어요!\", 1],\n",
    "    [\"별로였어요. 추천하지 않습니다.\", 0],\n",
    "    [\"훌륭한 연출과 연기였습니다.\", 1],\n",
    "    [\"지루하고 재미없었어요.\", 0],\n",
    "    [\"최고의 영화 중 하나입니다!\", 1]\n",
    "]\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sample_data)\n",
    "\n",
    "print(f\"CSV 파일 생성: {csv_file}\")\n",
    "\n",
    "# CSV에서 데이터셋 생성\n",
    "csv_dataset = load_dataset(\"csv\", data_files=csv_file)\n",
    "print(\"CSV 데이터셋:\")\n",
    "print(csv_dataset)\n",
    "print(\"\\n첫 번째 샘플:\")\n",
    "print(csv_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파일에서 데이터셋 생성\n",
    "import json\n",
    "\n",
    "json_file = os.path.join(temp_dir, \"sample_data.json\")\n",
    "\n",
    "# JSON 데이터 생성 (JSONL 형식)\n",
    "json_data = [\n",
    "    {\"text\": \"멋진 영화였어요!\", \"label\": 1, \"rating\": 4.5},\n",
    "    {\"text\": \"시간 낭비였습니다.\", \"label\": 0, \"rating\": 2.0},\n",
    "    {\"text\": \"강력 추천합니다!\", \"label\": 1, \"rating\": 5.0}\n",
    "]\n",
    "\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    for item in json_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"JSON 파일 생성: {json_file}\")\n",
    "\n",
    "# JSON에서 데이터셋 생성\n",
    "json_dataset = load_dataset(\"json\", data_files=json_file)\n",
    "print(\"JSON 데이터셋:\")\n",
    "print(json_dataset)\n",
    "print(\"\\n샘플:\")\n",
    "for i in range(len(json_dataset[\"train\"])):\n",
    "    print(json_dataset[\"train\"][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7070a24",
   "metadata": {},
   "source": [
    "### 3.2 Python 딕셔너리에서 데이터셋 생성\n",
    "\n",
    "메모리에 있는 Python 데이터로부터 직접 데이터셋을 생성할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c50270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_dict() 방법\n",
    "data_dict = {\n",
    "    \"pokemon\": [\"피카츄\", \"파이리\", \"꼬부기\", \"이상해씨\"],\n",
    "    \"type\": [\"전기\", \"불\", \"물\", \"풀\"],\n",
    "    \"power\": [90, 85, 80, 82]\n",
    "}\n",
    "\n",
    "dict_dataset = Dataset.from_dict(data_dict)\n",
    "print(\"딕셔너리에서 생성한 데이터셋:\")\n",
    "print(dict_dataset)\n",
    "print(\"\\n샘플:\")\n",
    "for i in range(len(dict_dataset)):\n",
    "    print(dict_dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_generator() 방법 - 메모리 효율적\n",
    "def data_generator():\n",
    "    \"\"\"대용량 데이터에 유용한 제네레이터\"\"\"\n",
    "    for i in range(10):\n",
    "        yield {\n",
    "            \"id\": i,\n",
    "            \"text\": f\"이것은 {i}번째 텍스트입니다.\",\n",
    "            \"score\": i * 0.1\n",
    "        }\n",
    "\n",
    "# Dataset 생성\n",
    "gen_dataset = Dataset.from_generator(data_generator)\n",
    "print(\"제네레이터에서 생성한 데이터셋:\")\n",
    "print(gen_dataset)\n",
    "print(\"\\n처음 3개 샘플:\")\n",
    "for i in range(3):\n",
    "    print(gen_dataset[i])\n",
    "\n",
    "# IterableDataset 생성 (스트리밍용)\n",
    "iterable_dataset = IterableDataset.from_generator(data_generator)\n",
    "print(\"\\n\\n스트리밍 데이터셋:\")\n",
    "print(iterable_dataset)\n",
    "print(\"스트리밍 샘플 (처음 3개):\")\n",
    "for i, sample in enumerate(iterable_dataset):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fde686",
   "metadata": {},
   "source": [
    "### 3.3 폴더 기반 빌더 (ImageFolder & AudioFolder)\n",
    "\n",
    "이미지나 오디오 데이터를 폴더 구조로 구성하여 데이터셋을 만들 수 있습니다.\n",
    "\n",
    "**폴더 구조 예시:**\n",
    "```\n",
    "data/\n",
    "├── train/\n",
    "│   ├── cats/\n",
    "│   │   ├── cat1.jpg\n",
    "│   │   └── cat2.jpg\n",
    "│   └── dogs/\n",
    "│       ├── dog1.jpg\n",
    "│       └── dog2.jpg\n",
    "└── test/\n",
    "    ├── cats/\n",
    "    └── dogs/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 기반 데이터셋 예시 (실제로는 이미지 파일이 필요)\n",
    "# dataset = load_dataset(\"imagefolder\", data_dir=\"path/to/image/folder\")\n",
    "\n",
    "# 메타데이터와 함께 사용하는 예시\n",
    "print(\"ImageFolder 사용 방법:\")\n",
    "print(\"\"\"\n",
    "# 이미지 데이터셋 로드\n",
    "image_dataset = load_dataset(\"imagefolder\", data_dir=\"path/to/images\")\n",
    "\n",
    "# 메타데이터가 있는 경우 (metadata.csv 파일 필요)\n",
    "# metadata.csv 내용:\n",
    "# file_name,caption\n",
    "# cat1.jpg,귀여운 고양이\n",
    "# dog1.jpg,활발한 강아지\n",
    "\n",
    "image_dataset = load_dataset(\"imagefolder\", data_dir=\"path/to/images\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nAudioFolder 사용 방법:\")\n",
    "print(\"\"\"\n",
    "# 오디오 데이터셋 로드\n",
    "audio_dataset = load_dataset(\"audiofolder\", data_dir=\"path/to/audio\")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f3081c",
   "metadata": {},
   "source": [
    "## 4. 데이터셋 활용하기\n",
    "\n",
    "로드한 데이터셋을 다양한 방법으로 조작하고 활용해봅시다.\n",
    "\n",
    "### 4.1 기본 데이터셋 조작\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fa8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 기본 조작 예제\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:1000]\")  # 처음 1000개만 로드\n",
    "\n",
    "print(\"=== 데이터셋 정보 ===\")\n",
    "print(f\"크기: {len(dataset)}\")\n",
    "print(f\"컬럼: {dataset.column_names}\")\n",
    "print(f\"특성: {dataset.features}\")\n",
    "\n",
    "# 인덱싱\n",
    "print(\"\\n=== 인덱싱 ===\")\n",
    "print(\"첫 번째 샘플:\", dataset[0][\"text\"][:100] + \"...\")\n",
    "print(\"처음 3개 라벨:\", dataset[:3][\"label\"])\n",
    "\n",
    "# 슬라이싱\n",
    "print(\"\\n=== 슬라이싱 ===\")\n",
    "subset = dataset[100:110]\n",
    "print(f\"100-110번째 샘플 개수: {len(subset['text'])}\")\n",
    "\n",
    "# 셔플\n",
    "print(\"\\n=== 셔플 ===\")\n",
    "shuffled = dataset.shuffle(seed=42)\n",
    "print(\"셔플 전 첫 번째 라벨:\", dataset[0][\"label\"])\n",
    "print(\"셔플 후 첫 번째 라벨:\", shuffled[0][\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터링\n",
    "print(\"=== 필터링 ===\")\n",
    "positive_reviews = dataset.filter(lambda example: example[\"label\"] == 1)\n",
    "print(f\"긍정 리뷰 개수: {len(positive_reviews)}\")\n",
    "\n",
    "# 정렬\n",
    "print(\"\\n=== 정렬 ===\")\n",
    "sorted_by_length = dataset.sort(\"text\", key=lambda x: len(x))\n",
    "print(\"가장 짧은 리뷰:\", sorted_by_length[0][\"text\"])\n",
    "print(\"가장 긴 리뷰 길이:\", len(sorted_by_length[-1][\"text\"]))\n",
    "\n",
    "# 선택 (select)\n",
    "print(\"\\n=== 선택 ===\")\n",
    "sample_100 = dataset.select(range(100))\n",
    "print(f\"선택된 샘플 개수: {len(sample_100)}\")\n",
    "\n",
    "# 데이터셋 분할\n",
    "print(\"\\n=== 데이터셋 분할 ===\")\n",
    "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "print(f\"훈련 데이터: {len(train_test['train'])}\")\n",
    "print(f\"테스트 데이터: {len(train_test['test'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009303b",
   "metadata": {},
   "source": [
    "## 5. 데이터 전처리\n",
    "\n",
    "### 5.1 map() 함수를 이용한 데이터 변환\n",
    "\n",
    "`map()` 함수는 데이터셋의 모든 요소에 함수를 적용하는 가장 중요한 메서드입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932874fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리 함수 정의\n",
    "def preprocess_text(examples):\n",
    "    \"\"\"텍스트를 소문자로 변환하고 길이 정보 추가\"\"\"\n",
    "    return {\n",
    "        \"text_lower\": [text.lower() for text in examples[\"text\"]],\n",
    "        \"text_length\": [len(text) for text in examples[\"text\"]]\n",
    "    }\n",
    "\n",
    "# 작은 데이터셋으로 테스트\n",
    "small_dataset = dataset.select(range(10))\n",
    "\n",
    "# map 적용 (배치 처리)\n",
    "processed = small_dataset.map(preprocess_text, batched=True)\n",
    "\n",
    "print(\"=== 전처리 결과 ===\")\n",
    "for i in range(3):\n",
    "    print(f\"원본: {processed[i]['text'][:50]}...\")\n",
    "    print(f\"소문자: {processed[i]['text_lower'][:50]}...\")\n",
    "    print(f\"길이: {processed[i]['text_length']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7ca2c",
   "metadata": {},
   "source": [
    "### 5.2 토크나이저를 이용한 텍스트 토큰화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507539f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 로드\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"텍스트를 토큰화하는 함수\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# 토큰화 적용\n",
    "tokenized_dataset = small_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"=== 토큰화 결과 ===\")\n",
    "print(\"새로운 컬럼들:\", tokenized_dataset.column_names)\n",
    "print()\n",
    "\n",
    "# 첫 번째 샘플 확인\n",
    "sample = tokenized_dataset[0]\n",
    "print(\"원본 텍스트:\", sample[\"text\"][:100] + \"...\")\n",
    "print(\"토큰 ID 개수:\", len(sample[\"input_ids\"]))\n",
    "print(\"토큰 ID (처음 10개):\", sample[\"input_ids\"][:10])\n",
    "print(\"어텐션 마스크 (처음 10개):\", sample[\"attention_mask\"][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53e0d2",
   "metadata": {},
   "source": [
    "### 5.3 PyTorch와 TensorFlow와의 호환성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba59c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 형식으로 변환\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "print(\"=== PyTorch 형식 ===\")\n",
    "print(\"데이터 형식:\", tokenized_dataset.format)\n",
    "sample = tokenized_dataset[0]\n",
    "print(\"input_ids 타입:\", type(sample[\"input_ids\"]))\n",
    "print(\"input_ids 형태:\", sample[\"input_ids\"].shape)\n",
    "\n",
    "# 원래 형식으로 되돌리기\n",
    "tokenized_dataset.reset_format()\n",
    "print(\"\\n=== 원래 형식으로 복원 ===\")\n",
    "print(\"데이터 형식:\", tokenized_dataset.format)\n",
    "\n",
    "# 컬럼 제거/추가\n",
    "print(\"\\n=== 컬럼 조작 ===\")\n",
    "# 불필요한 컬럼 제거\n",
    "clean_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "print(\"제거 후 컬럼:\", clean_dataset.column_names)\n",
    "\n",
    "# 새 컬럼 추가\n",
    "def add_ids(examples):\n",
    "    return {\"id\": list(range(len(examples[\"label\"])))}\n",
    "\n",
    "dataset_with_ids = clean_dataset.map(add_ids, batched=True)\n",
    "print(\"추가 후 컬럼:\", dataset_with_ids.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f991d32",
   "metadata": {},
   "source": [
    "### 5.4 스트리밍 데이터셋\n",
    "\n",
    "대용량 데이터셋은 스트리밍 방식으로 처리하여 메모리 사용량을 줄일 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 데이터셋 로드\n",
    "streaming_dataset = load_dataset(\"imdb\", streaming=True)\n",
    "\n",
    "print(\"=== 스트리밍 데이터셋 ===\")\n",
    "print(\"타입:\", type(streaming_dataset[\"train\"]))\n",
    "\n",
    "# 스트리밍으로 데이터 처리\n",
    "print(\"\\n처음 5개 샘플 (스트리밍):\")\n",
    "for i, sample in enumerate(streaming_dataset[\"train\"]):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"샘플 {i}: {sample['text'][:50]}... (라벨: {sample['label']})\")\n",
    "\n",
    "# 스트리밍 데이터셋에 map 적용\n",
    "def simple_preprocess(example):\n",
    "    example[\"text_length\"] = len(example[\"text\"])\n",
    "    return example\n",
    "\n",
    "processed_streaming = streaming_dataset[\"train\"].map(simple_preprocess)\n",
    "\n",
    "print(\"\\n전처리된 스트리밍 데이터:\")\n",
    "for i, sample in enumerate(processed_streaming):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"길이: {sample['text_length']}, 텍스트: {sample['text'][:30]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e179afc",
   "metadata": {},
   "source": [
    "## 6. 데이터셋 공유하기\n",
    "\n",
    "### 6.1 로컬에서 허브로 업로드\n",
    "\n",
    "생성한 데이터셋을 Hugging Face Hub에 업로드하여 공유할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 허브에 업로드하는 방법 (실제로는 인증이 필요)\n",
    "\n",
    "# 1. 로그인 (터미널에서 huggingface-cli login 실행)\n",
    "print(\"데이터셋 업로드 방법:\")\n",
    "print(\"\"\"\n",
    "# 1. 허브 로그인\n",
    "from huggingface_hub import login\n",
    "login()  # 또는 터미널에서 huggingface-cli login\n",
    "\n",
    "# 2. 데이터셋 업로드\n",
    "my_dataset = Dataset.from_dict({\n",
    "    \"text\": [\"안녕하세요\", \"반갑습니다\", \"감사합니다\"],\n",
    "    \"label\": [0, 1, 0]\n",
    "})\n",
    "\n",
    "# 허브에 업로드\n",
    "my_dataset.push_to_hub(\"username/my-dataset\")\n",
    "\n",
    "# 여러 스플릿이 있는 경우\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "dataset_dict.push_to_hub(\"username/my-dataset\")\n",
    "\"\"\")\n",
    "\n",
    "# 로컬에 저장\n",
    "print(\"\\n로컬 저장 방법:\")\n",
    "sample_dataset = Dataset.from_dict({\n",
    "    \"text\": [\"샘플 텍스트 1\", \"샘플 텍스트 2\"],\n",
    "    \"label\": [0, 1]\n",
    "})\n",
    "\n",
    "# 로컬에 저장\n",
    "save_path = os.path.join(temp_dir, \"my_dataset\")\n",
    "sample_dataset.save_to_disk(save_path)\n",
    "print(f\"데이터셋 저장 완료: {save_path}\")\n",
    "\n",
    "# 로컬에서 로드\n",
    "loaded_dataset = Dataset.load_from_disk(save_path)\n",
    "print(\"로드된 데이터셋:\", loaded_dataset)\n",
    "print(\"샘플:\", loaded_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d7c91",
   "metadata": {},
   "source": [
    "### 6.2 데이터셋 카드 작성\n",
    "\n",
    "데이터셋을 공유할 때는 `README.md` 파일에 데이터셋 카드를 작성하여 사용자들에게 정보를 제공해야 합니다.\n",
    "\n",
    "**데이터셋 카드 예시:**\n",
    "```markdown\n",
    "---\n",
    "license: apache-2.0\n",
    "task_categories:\n",
    "- text-classification\n",
    "language:\n",
    "- ko\n",
    "---\n",
    "\n",
    "# 한국어 감정 분석 데이터셋\n",
    "\n",
    "## 데이터셋 설명\n",
    "이 데이터셋은 한국어 리뷰 데이터의 감정을 분류하기 위한 데이터셋입니다.\n",
    "\n",
    "## 데이터셋 구조\n",
    "- **train**: 8,000개 샘플\n",
    "- **test**: 2,000개 샘플\n",
    "\n",
    "## 사용법\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"username/korean-sentiment\")\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c541e78",
   "metadata": {},
   "source": [
    "## 7. 실습 과제\n",
    "\n",
    "### 과제 1: 나만의 텍스트 분류 데이터셋 생성\n",
    "1. 영화 리뷰 데이터 10개를 만들어 CSV 파일로 저장\n",
    "2. 데이터셋으로 로드하고 train/test 분할 (8:2 비율)\n",
    "3. 텍스트를 토큰화하고 PyTorch 형식으로 변환\n",
    "\n",
    "### 과제 2: 데이터 전처리 파이프라인 구축\n",
    "1. 텍스트 길이 정보 추가\n",
    "2. 짧은 텍스트 필터링 (50자 이하 제거)\n",
    "3. 결과를 로컬에 저장\n",
    "\n",
    "### 과제 3: 스트리밍 데이터 처리\n",
    "1. 큰 데이터셋을 스트리밍으로 로드\n",
    "2. 배치 단위로 전처리 수행\n",
    "3. 처음 100개 샘플만 추출하여 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96262498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과제 1 해답 예시\n",
    "print(\"=== 과제 1: 나만의 데이터셋 생성 ===\")\n",
    "\n",
    "# 1. 영화 리뷰 데이터 생성\n",
    "movie_reviews = [\n",
    "    [\"이 영화는 정말 감동적이에요! 강력 추천합니다.\", 1],\n",
    "    [\"지루하고 예측 가능한 스토리였어요.\", 0],\n",
    "    [\"연출과 연기가 모두 훌륭했습니다.\", 1],\n",
    "    [\"시간 낭비였어요. 별로 추천하지 않아요.\", 0],\n",
    "    [\"최고의 영화 중 하나입니다!\", 1],\n",
    "    [\"스토리가 너무 복잡해서 이해하기 어려웠어요.\", 0],\n",
    "    [\"감동적인 결말이 인상 깊었습니다.\", 1],\n",
    "    [\"액션 시퀀스가 정말 대단했어요!\", 1],\n",
    "    [\"캐릭터들이 매력적이지 않았어요.\", 0],\n",
    "    [\"다시 보고 싶은 영화입니다.\", 1]\n",
    "]\n",
    "\n",
    "# CSV 파일로 저장\n",
    "review_file = os.path.join(temp_dir, \"movie_reviews.csv\")\n",
    "with open(review_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"review\", \"sentiment\"])\n",
    "    writer.writerows(movie_reviews)\n",
    "\n",
    "# 2. 데이터셋 로드 및 분할\n",
    "review_dataset = load_dataset(\"csv\", data_files=review_file)[\"train\"]\n",
    "train_test = review_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "print(f\"훈련 데이터: {len(train_test['train'])}개\")\n",
    "print(f\"테스트 데이터: {len(train_test['test'])}개\")\n",
    "\n",
    "# 3. 토큰화 및 PyTorch 형식 변환\n",
    "def tokenize_reviews(examples):\n",
    "    return tokenizer(examples[\"review\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "tokenized_reviews = train_test.map(tokenize_reviews, batched=True)\n",
    "tokenized_reviews.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"sentiment\"])\n",
    "\n",
    "print(\"토큰화 완료!\")\n",
    "print(\"훈련 데이터 첫 번째 샘플:\")\n",
    "print(f\"Shape: {tokenized_reviews['train'][0]['input_ids'].shape}\")\n",
    "print(f\"Label: {tokenized_reviews['train'][0]['sentiment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71449342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시 파일들 정리\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "print(\"임시 파일들이 정리되었습니다.\")\n",
    "\n",
    "print(\"\\n🎉 Hugging Face Datasets 강의를 마치겠습니다!\")\n",
    "print(\"\\n요약:\")\n",
    "print(\"✅ 허브에서 데이터셋 로드하기\")\n",
    "print(\"✅ 다양한 방법으로 데이터셋 생성하기\") \n",
    "print(\"✅ 데이터 전처리 및 변환\")\n",
    "print(\"✅ 스트리밍 데이터셋 활용\")\n",
    "print(\"✅ 데이터셋 저장 및 공유\")\n",
    "print(\"\\n추가 학습 자료:\")\n",
    "print(\"- 공식 문서: https://huggingface.co/docs/datasets\")\n",
    "print(\"- Hub 둘러보기: https://huggingface.co/datasets\")\n",
    "print(\"- 커뮤니티 포럼: https://discuss.huggingface.co\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
