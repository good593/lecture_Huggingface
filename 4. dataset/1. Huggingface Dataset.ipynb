{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbeed8e7",
   "metadata": {},
   "source": [
    "# ğŸ¤— Hugging Face Datasets ì™„ì „ì •ë³µ\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œ](#1-datasets-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì†Œê°œ)\n",
    "2. [í—ˆë¸Œì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œí•˜ê¸°](#2-í—ˆë¸Œì—ì„œ-ë°ì´í„°ì…‹-ë¡œë“œí•˜ê¸°)\n",
    "3. [ë°ì´í„°ì…‹ ìƒì„±í•˜ê¸°](#3-ë°ì´í„°ì…‹-ìƒì„±í•˜ê¸°)\n",
    "4. [ë°ì´í„°ì…‹ í™œìš©í•˜ê¸°](#4-ë°ì´í„°ì…‹-í™œìš©í•˜ê¸°)\n",
    "5. [ë°ì´í„° ì „ì²˜ë¦¬](#5-ë°ì´í„°-ì „ì²˜ë¦¬)\n",
    "6. [ë°ì´í„°ì…‹ ê³µìœ í•˜ê¸°](#6-ë°ì´í„°ì…‹-ê³µìœ í•˜ê¸°)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œ\n",
    "\n",
    "ğŸ¤— DatasetsëŠ” Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ë°ì´í„°ì…‹ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” íŠ¹ì§•\n",
    "- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: Apache Arrow ê¸°ë°˜ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬\n",
    "- **ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„**: ìµœì í™”ëœ ë°ì´í„° ë¡œë”© ë° ì²˜ë¦¬\n",
    "- **ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ ë“±\n",
    "- **ìŠ¤íŠ¸ë¦¬ë° ì§€ì›**: í° ë°ì´í„°ì…‹ì„ ë©”ëª¨ë¦¬ì— ëª¨ë‘ ë¡œë“œí•˜ì§€ ì•Šê³  ì²˜ë¦¬ ê°€ëŠ¥\n",
    "- **Hub ì—°ë™**: Hugging Face Hubì˜ ìˆ˜ì²œ ê°œ ë°ì´í„°ì…‹ì— ì‰½ê²Œ ì ‘ê·¼\n",
    "\n",
    "### ì„¤ì¹˜ ë°©ë²•\n",
    "```bash\n",
    "pip install datasets\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° import\n",
    "%pip install datasets transformers torch\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names, get_dataset_config_names\n",
    "from datasets import Dataset, DatasetDict, IterableDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Datasets ë²„ì „: {datasets.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596ff89",
   "metadata": {},
   "source": [
    "## 2. í—ˆë¸Œì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œí•˜ê¸°\n",
    "\n",
    "Hugging Face Hubì—ëŠ” NLP, Computer Vision, Audio ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ë°ì´í„°ì…‹ì´ ìˆ˜ì²œ ê°œ ì œê³µë©ë‹ˆë‹¤.\n",
    "\n",
    "### 2.1 ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸í•˜ê¸°\n",
    "\n",
    "ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê¸° ì „ì— `load_dataset_builder()`ë¥¼ ì‚¬ìš©í•´ ì •ë³´ë¥¼ ë¯¸ë¦¬ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1576e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸í•˜ê¸°\n",
    "ds_builder = load_dataset_builder(\"imdb\")\n",
    "\n",
    "print(\"=== ë°ì´í„°ì…‹ ì„¤ëª… ===\")\n",
    "print(ds_builder.info.description[:300] + \"...\")\n",
    "print()\n",
    "\n",
    "print(\"=== ë°ì´í„°ì…‹ íŠ¹ì„±(Features) ===\")\n",
    "print(ds_builder.info.features)\n",
    "print()\n",
    "\n",
    "print(\"=== ë°ì´í„°ì…‹ í¬ê¸° ===\")\n",
    "print(f\"ì´ í¬ê¸°: {ds_builder.info.dataset_size}\")\n",
    "print(f\"ë‹¤ìš´ë¡œë“œ í¬ê¸°: {ds_builder.info.download_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e443907",
   "metadata": {},
   "source": [
    "### 2.2 ìŠ¤í”Œë¦¿(Split) í™•ì¸í•˜ê¸°\n",
    "\n",
    "ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ì…‹ì€ train, validation, testë¡œ ë‚˜ë‰˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ì˜ ìŠ¤í”Œë¦¿ í™•ì¸\n",
    "splits = get_dataset_split_names(\"imdb\")\n",
    "print(\"ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤í”Œë¦¿:\", splits)\n",
    "\n",
    "# íŠ¹ì • ìŠ¤í”Œë¦¿ë§Œ ë¡œë“œ\n",
    "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "print(f\"\\ní›ˆë ¨ ë°ì´í„°ì…‹: {train_dataset}\")\n",
    "print(f\"ë°ì´í„° ê°œìˆ˜: {len(train_dataset)}\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\n=== ì²« ë²ˆì§¸ ìƒ˜í”Œ ===\")\n",
    "print(f\"í…ìŠ¤íŠ¸: {train_dataset[0]['text'][:200]}...\")\n",
    "print(f\"ë¼ë²¨: {train_dataset[0]['label']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ (ëª¨ë“  ìŠ¤í”Œë¦¿)\n",
    "full_dataset = load_dataset(\"imdb\")\n",
    "print(\"ì „ì²´ ë°ì´í„°ì…‹:\")\n",
    "print(full_dataset)\n",
    "print()\n",
    "\n",
    "# ê° ìŠ¤í”Œë¦¿ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
    "for split_name, split_data in full_dataset.items():\n",
    "    print(f\"{split_name}: {len(split_data):,}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec659a0c",
   "metadata": {},
   "source": [
    "### 2.3 êµ¬ì„±(Configuration) í™•ì¸í•˜ê¸°\n",
    "\n",
    "ì¼ë¶€ ë°ì´í„°ì…‹ì€ ì—¬ëŸ¬ í•˜ìœ„ ë°ì´í„°ì…‹(êµ¬ì„±)ì„ í¬í•¨í•©ë‹ˆë‹¤. ì˜ˆ: ë‹¤êµ­ì–´ ë°ì´í„°ì…‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819544bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤êµ­ì–´ ë°ì´í„°ì…‹ ì˜ˆì œ - PolyAI/minds14\n",
    "try:\n",
    "    configs = get_dataset_config_names(\"PolyAI/minds14\")\n",
    "    print(\"PolyAI/minds14 ë°ì´í„°ì…‹ì˜ êµ¬ì„±ë“¤:\")\n",
    "    print(configs[:10])  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n",
    "    \n",
    "    # íŠ¹ì • ì–¸ì–´ êµ¬ì„± ë¡œë“œ\n",
    "    minds_en = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train[:100]\")  # ì²˜ìŒ 100ê°œë§Œ\n",
    "    print(f\"\\nì˜ì–´(en-US) êµ¬ì„±: {minds_en}\")\n",
    "    print(f\"ì²« ë²ˆì§¸ ìƒ˜í”Œ: {minds_en[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "    print(\"ë‹¤ë¥¸ ì˜ˆì œë¡œ ì§„í–‰í•©ë‹ˆë‹¤...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c825ed",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„°ì…‹ ìƒì„±í•˜ê¸°\n",
    "\n",
    "ìì‹ ë§Œì˜ ë°ì´í„°ë¡œ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ì‹œë‹¤.\n",
    "\n",
    "### 3.1 íŒŒì¼ ê¸°ë°˜ ë¹Œë”\n",
    "\n",
    "CSV, JSON, Parquet, TXT ë“± ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ CSV ë°ì´í„° ìƒì„±\n",
    "import csv\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# ì„ì‹œ CSV íŒŒì¼ ìƒì„±\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "csv_file = os.path.join(temp_dir, \"sample_data.csv\")\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„° ì‘ì„±\n",
    "sample_data = [\n",
    "    [\"text\", \"label\"],\n",
    "    [\"ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆì–´ìš”!\", 1],\n",
    "    [\"ë³„ë¡œì˜€ì–´ìš”. ì¶”ì²œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\", 0],\n",
    "    [\"í›Œë¥­í•œ ì—°ì¶œê³¼ ì—°ê¸°ì˜€ìŠµë‹ˆë‹¤.\", 1],\n",
    "    [\"ì§€ë£¨í•˜ê³  ì¬ë¯¸ì—†ì—ˆì–´ìš”.\", 0],\n",
    "    [\"ìµœê³ ì˜ ì˜í™” ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤!\", 1]\n",
    "]\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sample_data)\n",
    "\n",
    "print(f\"CSV íŒŒì¼ ìƒì„±: {csv_file}\")\n",
    "\n",
    "# CSVì—ì„œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "csv_dataset = load_dataset(\"csv\", data_files=csv_file)\n",
    "print(\"CSV ë°ì´í„°ì…‹:\")\n",
    "print(csv_dataset)\n",
    "print(\"\\nì²« ë²ˆì§¸ ìƒ˜í”Œ:\")\n",
    "print(csv_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "import json\n",
    "\n",
    "json_file = os.path.join(temp_dir, \"sample_data.json\")\n",
    "\n",
    "# JSON ë°ì´í„° ìƒì„± (JSONL í˜•ì‹)\n",
    "json_data = [\n",
    "    {\"text\": \"ë©‹ì§„ ì˜í™”ì˜€ì–´ìš”!\", \"label\": 1, \"rating\": 4.5},\n",
    "    {\"text\": \"ì‹œê°„ ë‚­ë¹„ì˜€ìŠµë‹ˆë‹¤.\", \"label\": 0, \"rating\": 2.0},\n",
    "    {\"text\": \"ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤!\", \"label\": 1, \"rating\": 5.0}\n",
    "]\n",
    "\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    for item in json_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"JSON íŒŒì¼ ìƒì„±: {json_file}\")\n",
    "\n",
    "# JSONì—ì„œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "json_dataset = load_dataset(\"json\", data_files=json_file)\n",
    "print(\"JSON ë°ì´í„°ì…‹:\")\n",
    "print(json_dataset)\n",
    "print(\"\\nìƒ˜í”Œ:\")\n",
    "for i in range(len(json_dataset[\"train\"])):\n",
    "    print(json_dataset[\"train\"][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7070a24",
   "metadata": {},
   "source": [
    "### 3.2 Python ë”•ì…”ë„ˆë¦¬ì—ì„œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "\n",
    "ë©”ëª¨ë¦¬ì— ìˆëŠ” Python ë°ì´í„°ë¡œë¶€í„° ì§ì ‘ ë°ì´í„°ì…‹ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c50270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_dict() ë°©ë²•\n",
    "data_dict = {\n",
    "    \"pokemon\": [\"í”¼ì¹´ì¸„\", \"íŒŒì´ë¦¬\", \"ê¼¬ë¶€ê¸°\", \"ì´ìƒí•´ì”¨\"],\n",
    "    \"type\": [\"ì „ê¸°\", \"ë¶ˆ\", \"ë¬¼\", \"í’€\"],\n",
    "    \"power\": [90, 85, 80, 82]\n",
    "}\n",
    "\n",
    "dict_dataset = Dataset.from_dict(data_dict)\n",
    "print(\"ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±í•œ ë°ì´í„°ì…‹:\")\n",
    "print(dict_dataset)\n",
    "print(\"\\nìƒ˜í”Œ:\")\n",
    "for i in range(len(dict_dataset)):\n",
    "    print(dict_dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_generator() ë°©ë²• - ë©”ëª¨ë¦¬ íš¨ìœ¨ì \n",
    "def data_generator():\n",
    "    \"\"\"ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ìœ ìš©í•œ ì œë„¤ë ˆì´í„°\"\"\"\n",
    "    for i in range(10):\n",
    "        yield {\n",
    "            \"id\": i,\n",
    "            \"text\": f\"ì´ê²ƒì€ {i}ë²ˆì§¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\",\n",
    "            \"score\": i * 0.1\n",
    "        }\n",
    "\n",
    "# Dataset ìƒì„±\n",
    "gen_dataset = Dataset.from_generator(data_generator)\n",
    "print(\"ì œë„¤ë ˆì´í„°ì—ì„œ ìƒì„±í•œ ë°ì´í„°ì…‹:\")\n",
    "print(gen_dataset)\n",
    "print(\"\\nì²˜ìŒ 3ê°œ ìƒ˜í”Œ:\")\n",
    "for i in range(3):\n",
    "    print(gen_dataset[i])\n",
    "\n",
    "# IterableDataset ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°ìš©)\n",
    "iterable_dataset = IterableDataset.from_generator(data_generator)\n",
    "print(\"\\n\\nìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹:\")\n",
    "print(iterable_dataset)\n",
    "print(\"ìŠ¤íŠ¸ë¦¬ë° ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):\")\n",
    "for i, sample in enumerate(iterable_dataset):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fde686",
   "metadata": {},
   "source": [
    "### 3.3 í´ë” ê¸°ë°˜ ë¹Œë” (ImageFolder & AudioFolder)\n",
    "\n",
    "ì´ë¯¸ì§€ë‚˜ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ í´ë” êµ¬ì¡°ë¡œ êµ¬ì„±í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**í´ë” êµ¬ì¡° ì˜ˆì‹œ:**\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â”œâ”€â”€ cats/\n",
    "â”‚   â”‚   â”œâ”€â”€ cat1.jpg\n",
    "â”‚   â”‚   â””â”€â”€ cat2.jpg\n",
    "â”‚   â””â”€â”€ dogs/\n",
    "â”‚       â”œâ”€â”€ dog1.jpg\n",
    "â”‚       â””â”€â”€ dog2.jpg\n",
    "â””â”€â”€ test/\n",
    "    â”œâ”€â”€ cats/\n",
    "    â””â”€â”€ dogs/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í´ë” ê¸°ë°˜ ë°ì´í„°ì…‹ ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” ì´ë¯¸ì§€ íŒŒì¼ì´ í•„ìš”)\n",
    "# dataset = load_dataset(\"imagefolder\", data_dir=\"path/to/image/folder\")\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ\n",
    "print(\"ImageFolder ì‚¬ìš© ë°©ë²•:\")\n",
    "print(\"\"\"\n",
    "# ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "image_dataset = load_dataset(\"imagefolder\", data_dir=\"path/to/images\")\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° (metadata.csv íŒŒì¼ í•„ìš”)\n",
    "# metadata.csv ë‚´ìš©:\n",
    "# file_name,caption\n",
    "# cat1.jpg,ê·€ì—¬ìš´ ê³ ì–‘ì´\n",
    "# dog1.jpg,í™œë°œí•œ ê°•ì•„ì§€\n",
    "\n",
    "image_dataset = load_dataset(\"imagefolder\", data_dir=\"path/to/images\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nAudioFolder ì‚¬ìš© ë°©ë²•:\")\n",
    "print(\"\"\"\n",
    "# ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "audio_dataset = load_dataset(\"audiofolder\", data_dir=\"path/to/audio\")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f3081c",
   "metadata": {},
   "source": [
    "## 4. ë°ì´í„°ì…‹ í™œìš©í•˜ê¸°\n",
    "\n",
    "ë¡œë“œí•œ ë°ì´í„°ì…‹ì„ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì¡°ì‘í•˜ê³  í™œìš©í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "### 4.1 ê¸°ë³¸ ë°ì´í„°ì…‹ ì¡°ì‘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fa8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ê¸°ë³¸ ì¡°ì‘ ì˜ˆì œ\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:1000]\")  # ì²˜ìŒ 1000ê°œë§Œ ë¡œë“œ\n",
    "\n",
    "print(\"=== ë°ì´í„°ì…‹ ì •ë³´ ===\")\n",
    "print(f\"í¬ê¸°: {len(dataset)}\")\n",
    "print(f\"ì»¬ëŸ¼: {dataset.column_names}\")\n",
    "print(f\"íŠ¹ì„±: {dataset.features}\")\n",
    "\n",
    "# ì¸ë±ì‹±\n",
    "print(\"\\n=== ì¸ë±ì‹± ===\")\n",
    "print(\"ì²« ë²ˆì§¸ ìƒ˜í”Œ:\", dataset[0][\"text\"][:100] + \"...\")\n",
    "print(\"ì²˜ìŒ 3ê°œ ë¼ë²¨:\", dataset[:3][\"label\"])\n",
    "\n",
    "# ìŠ¬ë¼ì´ì‹±\n",
    "print(\"\\n=== ìŠ¬ë¼ì´ì‹± ===\")\n",
    "subset = dataset[100:110]\n",
    "print(f\"100-110ë²ˆì§¸ ìƒ˜í”Œ ê°œìˆ˜: {len(subset['text'])}\")\n",
    "\n",
    "# ì…”í”Œ\n",
    "print(\"\\n=== ì…”í”Œ ===\")\n",
    "shuffled = dataset.shuffle(seed=42)\n",
    "print(\"ì…”í”Œ ì „ ì²« ë²ˆì§¸ ë¼ë²¨:\", dataset[0][\"label\"])\n",
    "print(\"ì…”í”Œ í›„ ì²« ë²ˆì§¸ ë¼ë²¨:\", shuffled[0][\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„í„°ë§\n",
    "print(\"=== í•„í„°ë§ ===\")\n",
    "positive_reviews = dataset.filter(lambda example: example[\"label\"] == 1)\n",
    "print(f\"ê¸ì • ë¦¬ë·° ê°œìˆ˜: {len(positive_reviews)}\")\n",
    "\n",
    "# ì •ë ¬\n",
    "print(\"\\n=== ì •ë ¬ ===\")\n",
    "sorted_by_length = dataset.sort(\"text\", key=lambda x: len(x))\n",
    "print(\"ê°€ì¥ ì§§ì€ ë¦¬ë·°:\", sorted_by_length[0][\"text\"])\n",
    "print(\"ê°€ì¥ ê¸´ ë¦¬ë·° ê¸¸ì´:\", len(sorted_by_length[-1][\"text\"]))\n",
    "\n",
    "# ì„ íƒ (select)\n",
    "print(\"\\n=== ì„ íƒ ===\")\n",
    "sample_100 = dataset.select(range(100))\n",
    "print(f\"ì„ íƒëœ ìƒ˜í”Œ ê°œìˆ˜: {len(sample_100)}\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¶„í• \n",
    "print(\"\\n=== ë°ì´í„°ì…‹ ë¶„í•  ===\")\n",
    "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_test['train'])}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(train_test['test'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009303b",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "### 5.1 map() í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ë°ì´í„° ë³€í™˜\n",
    "\n",
    "`map()` í•¨ìˆ˜ëŠ” ë°ì´í„°ì…‹ì˜ ëª¨ë“  ìš”ì†Œì— í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë©”ì„œë“œì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932874fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def preprocess_text(examples):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ê¸¸ì´ ì •ë³´ ì¶”ê°€\"\"\"\n",
    "    return {\n",
    "        \"text_lower\": [text.lower() for text in examples[\"text\"]],\n",
    "        \"text_length\": [len(text) for text in examples[\"text\"]]\n",
    "    }\n",
    "\n",
    "# ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "small_dataset = dataset.select(range(10))\n",
    "\n",
    "# map ì ìš© (ë°°ì¹˜ ì²˜ë¦¬)\n",
    "processed = small_dataset.map(preprocess_text, batched=True)\n",
    "\n",
    "print(\"=== ì „ì²˜ë¦¬ ê²°ê³¼ ===\")\n",
    "for i in range(3):\n",
    "    print(f\"ì›ë³¸: {processed[i]['text'][:50]}...\")\n",
    "    print(f\"ì†Œë¬¸ì: {processed[i]['text_lower'][:50]}...\")\n",
    "    print(f\"ê¸¸ì´: {processed[i]['text_length']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7ca2c",
   "metadata": {},
   "source": [
    "### 5.2 í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ í† í°í™”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507539f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# í† í°í™” ì ìš©\n",
    "tokenized_dataset = small_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"=== í† í°í™” ê²°ê³¼ ===\")\n",
    "print(\"ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤:\", tokenized_dataset.column_names)\n",
    "print()\n",
    "\n",
    "# ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸\n",
    "sample = tokenized_dataset[0]\n",
    "print(\"ì›ë³¸ í…ìŠ¤íŠ¸:\", sample[\"text\"][:100] + \"...\")\n",
    "print(\"í† í° ID ê°œìˆ˜:\", len(sample[\"input_ids\"]))\n",
    "print(\"í† í° ID (ì²˜ìŒ 10ê°œ):\", sample[\"input_ids\"][:10])\n",
    "print(\"ì–´í…ì…˜ ë§ˆìŠ¤í¬ (ì²˜ìŒ 10ê°œ):\", sample[\"attention_mask\"][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53e0d2",
   "metadata": {},
   "source": [
    "### 5.3 PyTorchì™€ TensorFlowì™€ì˜ í˜¸í™˜ì„±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba59c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "print(\"=== PyTorch í˜•ì‹ ===\")\n",
    "print(\"ë°ì´í„° í˜•ì‹:\", tokenized_dataset.format)\n",
    "sample = tokenized_dataset[0]\n",
    "print(\"input_ids íƒ€ì…:\", type(sample[\"input_ids\"]))\n",
    "print(\"input_ids í˜•íƒœ:\", sample[\"input_ids\"].shape)\n",
    "\n",
    "# ì›ë˜ í˜•ì‹ìœ¼ë¡œ ë˜ëŒë¦¬ê¸°\n",
    "tokenized_dataset.reset_format()\n",
    "print(\"\\n=== ì›ë˜ í˜•ì‹ìœ¼ë¡œ ë³µì› ===\")\n",
    "print(\"ë°ì´í„° í˜•ì‹:\", tokenized_dataset.format)\n",
    "\n",
    "# ì»¬ëŸ¼ ì œê±°/ì¶”ê°€\n",
    "print(\"\\n=== ì»¬ëŸ¼ ì¡°ì‘ ===\")\n",
    "# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n",
    "clean_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "print(\"ì œê±° í›„ ì»¬ëŸ¼:\", clean_dataset.column_names)\n",
    "\n",
    "# ìƒˆ ì»¬ëŸ¼ ì¶”ê°€\n",
    "def add_ids(examples):\n",
    "    return {\"id\": list(range(len(examples[\"label\"])))}\n",
    "\n",
    "dataset_with_ids = clean_dataset.map(add_ids, batched=True)\n",
    "print(\"ì¶”ê°€ í›„ ì»¬ëŸ¼:\", dataset_with_ids.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f991d32",
   "metadata": {},
   "source": [
    "### 5.4 ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹\n",
    "\n",
    "ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì€ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "streaming_dataset = load_dataset(\"imdb\", streaming=True)\n",
    "\n",
    "print(\"=== ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ ===\")\n",
    "print(\"íƒ€ì…:\", type(streaming_dataset[\"train\"]))\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬\n",
    "print(\"\\nì²˜ìŒ 5ê°œ ìƒ˜í”Œ (ìŠ¤íŠ¸ë¦¬ë°):\")\n",
    "for i, sample in enumerate(streaming_dataset[\"train\"]):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"ìƒ˜í”Œ {i}: {sample['text'][:50]}... (ë¼ë²¨: {sample['label']})\")\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ì— map ì ìš©\n",
    "def simple_preprocess(example):\n",
    "    example[\"text_length\"] = len(example[\"text\"])\n",
    "    return example\n",
    "\n",
    "processed_streaming = streaming_dataset[\"train\"].map(simple_preprocess)\n",
    "\n",
    "print(\"\\nì „ì²˜ë¦¬ëœ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°:\")\n",
    "for i, sample in enumerate(processed_streaming):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"ê¸¸ì´: {sample['text_length']}, í…ìŠ¤íŠ¸: {sample['text'][:30]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e179afc",
   "metadata": {},
   "source": [
    "## 6. ë°ì´í„°ì…‹ ê³µìœ í•˜ê¸°\n",
    "\n",
    "### 6.1 ë¡œì»¬ì—ì„œ í—ˆë¸Œë¡œ ì—…ë¡œë“œ\n",
    "\n",
    "ìƒì„±í•œ ë°ì´í„°ì…‹ì„ Hugging Face Hubì— ì—…ë¡œë“œí•˜ì—¬ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ í—ˆë¸Œì— ì—…ë¡œë“œí•˜ëŠ” ë°©ë²• (ì‹¤ì œë¡œëŠ” ì¸ì¦ì´ í•„ìš”)\n",
    "\n",
    "# 1. ë¡œê·¸ì¸ (í„°ë¯¸ë„ì—ì„œ huggingface-cli login ì‹¤í–‰)\n",
    "print(\"ë°ì´í„°ì…‹ ì—…ë¡œë“œ ë°©ë²•:\")\n",
    "print(\"\"\"\n",
    "# 1. í—ˆë¸Œ ë¡œê·¸ì¸\n",
    "from huggingface_hub import login\n",
    "login()  # ë˜ëŠ” í„°ë¯¸ë„ì—ì„œ huggingface-cli login\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ ì—…ë¡œë“œ\n",
    "my_dataset = Dataset.from_dict({\n",
    "    \"text\": [\"ì•ˆë…•í•˜ì„¸ìš”\", \"ë°˜ê°‘ìŠµë‹ˆë‹¤\", \"ê°ì‚¬í•©ë‹ˆë‹¤\"],\n",
    "    \"label\": [0, 1, 0]\n",
    "})\n",
    "\n",
    "# í—ˆë¸Œì— ì—…ë¡œë“œ\n",
    "my_dataset.push_to_hub(\"username/my-dataset\")\n",
    "\n",
    "# ì—¬ëŸ¬ ìŠ¤í”Œë¦¿ì´ ìˆëŠ” ê²½ìš°\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "dataset_dict.push_to_hub(\"username/my-dataset\")\n",
    "\"\"\")\n",
    "\n",
    "# ë¡œì»¬ì— ì €ì¥\n",
    "print(\"\\në¡œì»¬ ì €ì¥ ë°©ë²•:\")\n",
    "sample_dataset = Dataset.from_dict({\n",
    "    \"text\": [\"ìƒ˜í”Œ í…ìŠ¤íŠ¸ 1\", \"ìƒ˜í”Œ í…ìŠ¤íŠ¸ 2\"],\n",
    "    \"label\": [0, 1]\n",
    "})\n",
    "\n",
    "# ë¡œì»¬ì— ì €ì¥\n",
    "save_path = os.path.join(temp_dir, \"my_dataset\")\n",
    "sample_dataset.save_to_disk(save_path)\n",
    "print(f\"ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "\n",
    "# ë¡œì»¬ì—ì„œ ë¡œë“œ\n",
    "loaded_dataset = Dataset.load_from_disk(save_path)\n",
    "print(\"ë¡œë“œëœ ë°ì´í„°ì…‹:\", loaded_dataset)\n",
    "print(\"ìƒ˜í”Œ:\", loaded_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d7c91",
   "metadata": {},
   "source": [
    "### 6.2 ë°ì´í„°ì…‹ ì¹´ë“œ ì‘ì„±\n",
    "\n",
    "ë°ì´í„°ì…‹ì„ ê³µìœ í•  ë•ŒëŠ” `README.md` íŒŒì¼ì— ë°ì´í„°ì…‹ ì¹´ë“œë¥¼ ì‘ì„±í•˜ì—¬ ì‚¬ìš©ìë“¤ì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë°ì´í„°ì…‹ ì¹´ë“œ ì˜ˆì‹œ:**\n",
    "```markdown\n",
    "---\n",
    "license: apache-2.0\n",
    "task_categories:\n",
    "- text-classification\n",
    "language:\n",
    "- ko\n",
    "---\n",
    "\n",
    "# í•œêµ­ì–´ ê°ì • ë¶„ì„ ë°ì´í„°ì…‹\n",
    "\n",
    "## ë°ì´í„°ì…‹ ì„¤ëª…\n",
    "ì´ ë°ì´í„°ì…‹ì€ í•œêµ­ì–´ ë¦¬ë·° ë°ì´í„°ì˜ ê°ì •ì„ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ë°ì´í„°ì…‹ êµ¬ì¡°\n",
    "- **train**: 8,000ê°œ ìƒ˜í”Œ\n",
    "- **test**: 2,000ê°œ ìƒ˜í”Œ\n",
    "\n",
    "## ì‚¬ìš©ë²•\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"username/korean-sentiment\")\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c541e78",
   "metadata": {},
   "source": [
    "## 7. ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "### ê³¼ì œ 1: ë‚˜ë§Œì˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë°ì´í„°ì…‹ ìƒì„±\n",
    "1. ì˜í™” ë¦¬ë·° ë°ì´í„° 10ê°œë¥¼ ë§Œë“¤ì–´ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "2. ë°ì´í„°ì…‹ìœ¼ë¡œ ë¡œë“œí•˜ê³  train/test ë¶„í•  (8:2 ë¹„ìœ¨)\n",
    "3. í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  PyTorch í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "\n",
    "### ê³¼ì œ 2: ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
    "1. í…ìŠ¤íŠ¸ ê¸¸ì´ ì •ë³´ ì¶”ê°€\n",
    "2. ì§§ì€ í…ìŠ¤íŠ¸ í•„í„°ë§ (50ì ì´í•˜ ì œê±°)\n",
    "3. ê²°ê³¼ë¥¼ ë¡œì»¬ì— ì €ì¥\n",
    "\n",
    "### ê³¼ì œ 3: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬\n",
    "1. í° ë°ì´í„°ì…‹ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¡œë“œ\n",
    "2. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì „ì²˜ë¦¬ ìˆ˜í–‰\n",
    "3. ì²˜ìŒ 100ê°œ ìƒ˜í”Œë§Œ ì¶”ì¶œí•˜ì—¬ ë¶„ì„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96262498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³¼ì œ 1 í•´ë‹µ ì˜ˆì‹œ\n",
    "print(\"=== ê³¼ì œ 1: ë‚˜ë§Œì˜ ë°ì´í„°ì…‹ ìƒì„± ===\")\n",
    "\n",
    "# 1. ì˜í™” ë¦¬ë·° ë°ì´í„° ìƒì„±\n",
    "movie_reviews = [\n",
    "    [\"ì´ ì˜í™”ëŠ” ì •ë§ ê°ë™ì ì´ì—ìš”! ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤.\", 1],\n",
    "    [\"ì§€ë£¨í•˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ìŠ¤í† ë¦¬ì˜€ì–´ìš”.\", 0],\n",
    "    [\"ì—°ì¶œê³¼ ì—°ê¸°ê°€ ëª¨ë‘ í›Œë¥­í–ˆìŠµë‹ˆë‹¤.\", 1],\n",
    "    [\"ì‹œê°„ ë‚­ë¹„ì˜€ì–´ìš”. ë³„ë¡œ ì¶”ì²œí•˜ì§€ ì•Šì•„ìš”.\", 0],\n",
    "    [\"ìµœê³ ì˜ ì˜í™” ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤!\", 1],\n",
    "    [\"ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ë³µì¡í•´ì„œ ì´í•´í•˜ê¸° ì–´ë ¤ì› ì–´ìš”.\", 0],\n",
    "    [\"ê°ë™ì ì¸ ê²°ë§ì´ ì¸ìƒ ê¹Šì—ˆìŠµë‹ˆë‹¤.\", 1],\n",
    "    [\"ì•¡ì…˜ ì‹œí€€ìŠ¤ê°€ ì •ë§ ëŒ€ë‹¨í–ˆì–´ìš”!\", 1],\n",
    "    [\"ìºë¦­í„°ë“¤ì´ ë§¤ë ¥ì ì´ì§€ ì•Šì•˜ì–´ìš”.\", 0],\n",
    "    [\"ë‹¤ì‹œ ë³´ê³  ì‹¶ì€ ì˜í™”ì…ë‹ˆë‹¤.\", 1]\n",
    "]\n",
    "\n",
    "# CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "review_file = os.path.join(temp_dir, \"movie_reviews.csv\")\n",
    "with open(review_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"review\", \"sentiment\"])\n",
    "    writer.writerows(movie_reviews)\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„í• \n",
    "review_dataset = load_dataset(\"csv\", data_files=review_file)[\"train\"]\n",
    "train_test = review_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_test['train'])}ê°œ\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(train_test['test'])}ê°œ\")\n",
    "\n",
    "# 3. í† í°í™” ë° PyTorch í˜•ì‹ ë³€í™˜\n",
    "def tokenize_reviews(examples):\n",
    "    return tokenizer(examples[\"review\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "tokenized_reviews = train_test.map(tokenize_reviews, batched=True)\n",
    "tokenized_reviews.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"sentiment\"])\n",
    "\n",
    "print(\"í† í°í™” ì™„ë£Œ!\")\n",
    "print(\"í›ˆë ¨ ë°ì´í„° ì²« ë²ˆì§¸ ìƒ˜í”Œ:\")\n",
    "print(f\"Shape: {tokenized_reviews['train'][0]['input_ids'].shape}\")\n",
    "print(f\"Label: {tokenized_reviews['train'][0]['sentiment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71449342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "print(\"ì„ì‹œ íŒŒì¼ë“¤ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\nğŸ‰ Hugging Face Datasets ê°•ì˜ë¥¼ ë§ˆì¹˜ê² ìŠµë‹ˆë‹¤!\")\n",
    "print(\"\\nìš”ì•½:\")\n",
    "print(\"âœ… í—ˆë¸Œì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œí•˜ê¸°\")\n",
    "print(\"âœ… ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±í•˜ê¸°\") \n",
    "print(\"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³€í™˜\")\n",
    "print(\"âœ… ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ í™œìš©\")\n",
    "print(\"âœ… ë°ì´í„°ì…‹ ì €ì¥ ë° ê³µìœ \")\n",
    "print(\"\\nì¶”ê°€ í•™ìŠµ ìë£Œ:\")\n",
    "print(\"- ê³µì‹ ë¬¸ì„œ: https://huggingface.co/docs/datasets\")\n",
    "print(\"- Hub ë‘˜ëŸ¬ë³´ê¸°: https://huggingface.co/datasets\")\n",
    "print(\"- ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼: https://discuss.huggingface.co\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
